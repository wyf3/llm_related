{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b15b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "\n",
    "from sympy import isprime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import normalizers, Regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124f9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EngramConfig:\n",
    "    tokenizer_name_or_path: str = \"/home/user/Downloads/Qwen2.5-0.5B-Instruct\"\n",
    "    engram_vocab_size: List[int] = field(default_factory=lambda: [151665*5, 151665*5]) # [2-gram vocab size, 3-gram vocab size]\n",
    "    max_ngram_size: int = 3 # 最多连续3个词作为一个词组\n",
    "    n_embed_per_ngram: int = 512\n",
    "    n_head_per_ngram: int = 8 # 每个ngram有几个hash头，减缓冲突\n",
    "    layer_ids: List[int] = field(default_factory=lambda: [1, 15]) # 在第几层添加engram模块\n",
    "    pad_id: int = 2\n",
    "    seed: int = 0\n",
    "    kernel_size: int = 4\n",
    "    \n",
    "@dataclass\n",
    "class BackBoneConfig:\n",
    "    hidden_size: int = 1024\n",
    "    hc_mult: int = 4\n",
    "    vocab_size: int = 151665\n",
    "    num_layers: int = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b319f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_cfg = EngramConfig()\n",
    "backbone_config = BackBoneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5cff1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_name_or_path,\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, trust_remote_code=True)\n",
    "        \n",
    "        SENTINEL = \"\\uE000\"\n",
    "        # 对原始词表进行标准化，转换小写，去掉重音符号，去掉空格等，用于词表压缩\n",
    "        self.normalizer = normalizers.Sequence([\n",
    "            normalizers.NFKC(),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.StripAccents(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.Replace(Regex(r\"[ \\t\\r\\n]+\"), \" \"),\n",
    "            normalizers.Replace(Regex(r\"^ $\"), SENTINEL),\n",
    "            normalizers.Strip(),\n",
    "            normalizers.Replace(SENTINEL, \" \"),\n",
    "        ])\n",
    "        \n",
    "        self.lookup_table, self.num_new_token = self._build_lookup_table()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_new_token\n",
    "    \n",
    "    # 将原始词表中的token_id映射到压缩后的token id\n",
    "    def _build_lookup_table(self):\n",
    "        old2new = {}\n",
    "        key2new = {}          \n",
    "        new_tokens = []\n",
    "\n",
    "        vocab_size = len(self.tokenizer)\n",
    "        for tid in range(vocab_size):\n",
    "            text = self.tokenizer.decode([tid], skip_special_tokens=False)\n",
    "            \n",
    "            if \"�\" in text:\n",
    "                key = self.tokenizer.convert_ids_to_tokens(tid)\n",
    "            else:\n",
    "                norm = self.normalizer.normalize_str(text)\n",
    "                key = norm if norm else text\n",
    "\n",
    "            nid = key2new.get(key)\n",
    "            if nid is None:\n",
    "                nid = len(new_tokens)\n",
    "                key2new[key] = nid\n",
    "                new_tokens.append(key)\n",
    "            old2new[tid] = nid\n",
    "        \n",
    "        lookup = np.empty(vocab_size, dtype=np.int64)\n",
    "        for tid in range(vocab_size):\n",
    "            lookup[tid] = old2new[tid]\n",
    "\n",
    "        return lookup, len(new_tokens)\n",
    "    \n",
    "    def _compress(self, input_ids):\n",
    "        arr = np.asarray(input_ids, dtype=np.int64)\n",
    "        pos_mask = arr >= 0\n",
    "        out = arr.copy()\n",
    "        valid_ids = arr[pos_mask]\n",
    "        out[pos_mask] = self.lookup_table[valid_ids]\n",
    "        return out   \n",
    "    \n",
    "    def __call__(self, input_ids):\n",
    "        return self._compress(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303ced84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_tokenizer = CompressedTokenizer(\n",
    "            tokenizer_name_or_path=engram_cfg.tokenizer_name_or_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3794c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "压缩后的词表大小: 107453\n",
      "原始词表大小: 151643\n",
      "压缩率: 0.29140810983691956\n"
     ]
    }
   ],
   "source": [
    "print('压缩后的词表大小:', len(compressed_tokenizer))\n",
    "print('原始词表大小:', compressed_tokenizer.tokenizer.vocab_size)\n",
    "print('压缩率:', 1 - len(compressed_tokenizer) / compressed_tokenizer.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3602a3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始input_ids: [14990, 1879, 11, 21927, 1879]\n",
      "压缩后的input_ids: [6378 1346   11 6378 1346]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = compressed_tokenizer.tokenizer.encode('hello world, Hello world')\n",
    "print('原始input_ids:', input_ids)\n",
    "compressedinput_ids = compressed_tokenizer(input_ids)\n",
    "print('压缩后的input_ids:', compressedinput_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61eda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortConv(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size: int, \n",
    "        kernel_size: int = 4, \n",
    "        dilation: int = 1, \n",
    "        norm_eps: float = 1e-5,\n",
    "        hc_mult: int = 4,\n",
    "        activation: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hc_mult = hc_mult\n",
    "        self.activation = activation\n",
    "        \n",
    "        total_channels = hidden_size * hc_mult\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=total_channels,\n",
    "            out_channels=total_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=total_channels,\n",
    "            bias=False,\n",
    "            padding=(kernel_size - 1) * dilation,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "\n",
    "        self.norms = nn.ModuleList([\n",
    "            nn.RMSNorm(hidden_size, eps=norm_eps) \n",
    "            for _ in range(hc_mult)\n",
    "        ])\n",
    "        \n",
    "        if self.activation:\n",
    "            self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input:  (B,L,HC_MULT,D)\n",
    "        Output: (B,L,HC_MULT,D)\n",
    "        \"\"\"\n",
    "        B, T, G, C = x.shape\n",
    "        \n",
    "        assert G == self.hc_mult, f\"Input groups {G} != hc_mult {self.hc_mult}\"\n",
    "\n",
    "        normed_chunks = []\n",
    "        for i in range(G):\n",
    "            chunk = x[:, :, i, :]\n",
    "            normed_chunks.append(self.norms[i](chunk))\n",
    "        \n",
    "        x_norm = torch.cat(normed_chunks, dim=-1)\n",
    "        x_bct = x_norm.transpose(1, 2)\n",
    "        y_bct = self.conv(x_bct)\n",
    "        y_bct = y_bct[..., :T]\n",
    "\n",
    "        if self.activation:\n",
    "            y_bct = self.act_fn(y_bct)\n",
    "        y = y_bct.transpose(1, 2).view(B, T, G, C).contiguous()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03f1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发现下一个质数\n",
    "def find_next_prime(start, seen_primes):\n",
    "    candidate = start + 1\n",
    "    while True:\n",
    "        if isprime(candidate) and candidate not in seen_primes:\n",
    "            return candidate\n",
    "        candidate += 1\n",
    "\n",
    "class NgramHashMapping:\n",
    "    def __init__(\n",
    "        self, \n",
    "        engram_vocab_size,\n",
    "        max_ngram_size,\n",
    "        n_embed_per_ngram,\n",
    "        n_head_per_ngram,\n",
    "        layer_ids,\n",
    "        tokenizer_name_or_path,\n",
    "        pad_id,\n",
    "        seed,  \n",
    "    ):\n",
    "        self.vocab_size_per_ngram = engram_vocab_size\n",
    "        self.max_ngram_size = max_ngram_size\n",
    "        self.n_embed_per_ngram = n_embed_per_ngram\n",
    "        self.n_head_per_ngram = n_head_per_ngram\n",
    "        self.pad_id = pad_id\n",
    "        self.layer_ids = layer_ids\n",
    "\n",
    "        self.compressed_tokenizer = CompressedTokenizer(\n",
    "            tokenizer_name_or_path=tokenizer_name_or_path\n",
    "        )            \n",
    "        self.tokenizer_vocab_size = len(self.compressed_tokenizer)\n",
    "        if self.pad_id is not None:\n",
    "            self.pad_id = int(self.compressed_tokenizer.lookup_table[self.pad_id])\n",
    "\n",
    "        \n",
    "        # int64能表示的最大值\n",
    "        max_long = np.iinfo(np.int64).max\n",
    "        # 除以压缩后词表大小（确保乘积不会溢出）\n",
    "        M_max = int(max_long // self.tokenizer_vocab_size)\n",
    "        # half_bound是后续生成随机数的上界，为了确保乘子是奇数（后续会2r+1），提前除以2，防止溢出\n",
    "        half_bound = max(1, M_max // 2)\n",
    "        PRIME_1 = 10007\n",
    "        \n",
    "        # 记录每一层不同ngram的乘子\n",
    "        self.layer_multipliers = {}\n",
    "\n",
    "        for layer_id in self.layer_ids:\n",
    "            # 为每一层生成一个随机数种子\n",
    "            base_seed = int(seed + PRIME_1 * int(layer_id))\n",
    "            g = np.random.default_rng(base_seed)\n",
    "            # 生成max_ngram_size个随机数，范围是[0, half_bound)\n",
    "            r = g.integers(\n",
    "                low=0,\n",
    "                high=half_bound,\n",
    "                size=(self.max_ngram_size,),\n",
    "                dtype=np.int64\n",
    "            )\n",
    "            # 确保乘子是奇数\n",
    "            multipliers = r * 2 + 1\n",
    "            self.layer_multipliers[layer_id] = multipliers\n",
    "\n",
    "        self.vocab_size_across_layers = self.calculate_vocab_size_across_layers()\n",
    "\n",
    "    def calculate_vocab_size_across_layers(self):\n",
    "        # 记录已生成的质数，防止重复\n",
    "        seen_primes = set()\n",
    "        \n",
    "        # 记录所有层所有ngram的不同头的词表大小\n",
    "        vocab_size_across_layers = {}\n",
    "        \n",
    "        for layer_id in self.layer_ids:\n",
    "            # 记录当前层所有ngram（2-gram、3-gram、...）的不同头的词表大小\n",
    "            all_ngram_vocab_sizes = []\n",
    "            for ngram in range(2, self.max_ngram_size + 1):\n",
    "                current_ngram_heads_sizes = []\n",
    "                # 获取ngram的基础词表大小\n",
    "                vocab_size = self.vocab_size_per_ngram[ngram - 2]\n",
    "                # ngram头数\n",
    "                num_head = self.n_head_per_ngram\n",
    "                # 从基础词表大小开始寻找下一个质数\n",
    "                current_prime_search_start = vocab_size - 1\n",
    "                \n",
    "                # 为每个头生成一个质数（这个质数比基础词表略大，可以理解成当前头对应的虚拟词表）\n",
    "                for _ in range(num_head):\n",
    "                    found_prime = find_next_prime(\n",
    "                        current_prime_search_start, \n",
    "                        seen_primes\n",
    "                    )\n",
    "                    seen_primes.add(found_prime)\n",
    "                    current_ngram_heads_sizes.append(found_prime)\n",
    "                    current_prime_search_start = found_prime\n",
    "                \n",
    "                all_ngram_vocab_sizes.append(current_ngram_heads_sizes)\n",
    "            vocab_size_across_layers[layer_id] = all_ngram_vocab_sizes\n",
    "            \n",
    "        return vocab_size_across_layers\n",
    "\n",
    "    def _get_ngram_hashes(\n",
    "        self,\n",
    "        input_ids: np.ndarray,\n",
    "        layer_id: int,\n",
    "    ) -> np.ndarray:\n",
    "        x = np.asarray(input_ids, dtype=np.int64)\n",
    "        B, T = x.shape\n",
    "\n",
    "        # 获取当前层所有ngram的乘子\n",
    "        multipliers = self.layer_multipliers[layer_id]\n",
    "\n",
    "        # 用于处理ngram的偏移\n",
    "        def shift_k(k: int) -> np.ndarray:\n",
    "            if k == 0: return x\n",
    "            shifted = np.pad(x, ((0, 0), (k, 0)),\n",
    "                                mode='constant', constant_values=self.pad_id)[:, :T]\n",
    "            return shifted\n",
    "\n",
    "        # 计算当前层所有ngram的偏移结果\n",
    "        base_shifts = [shift_k(k) for k in range(self.max_ngram_size)]\n",
    "\n",
    "        # all_hashes存储的是某一层所有ngram所有头的hash值\n",
    "        all_hashes = []\n",
    "        \n",
    "        for n in range(2, self.max_ngram_size + 1):\n",
    "            n_gram_index = n - 2\n",
    "            # 获取ngram对应的tokens\n",
    "            tokens = base_shifts[:n]\n",
    "            # 将ngram内的token_id乘以对应的乘子并异或，得到一个值mix（这个值可能会很大，如果对每个mix值生成一个embedding，范围将会非常大，几十亿甚至几百亿维的 embedding 表，根本存不下，所以需要对其取模压缩到固定范围内）\n",
    "            mix = (tokens[0] * multipliers[0])\n",
    "            for k in range(1, n):\n",
    "                mix = np.bitwise_xor(mix, tokens[k] * multipliers[k])\n",
    "            num_heads_for_this_ngram = self.n_head_per_ngram\n",
    "            head_vocab_sizes = self.vocab_size_across_layers[layer_id][n_gram_index]\n",
    "            \n",
    "            # 对mix值进行取模，模即是之前每个token对应的质数，即每个头的词表大小，这样就可以将mix值压缩到虚拟词表的范围内\n",
    "            # 为了减缓冲突，每个ngram会有多个头，每个头对应一个不同的质数\n",
    "            # 两个ngram只有当所有头对应的hash值都相等才会冲突，这个概率非常低\n",
    "            for j in range(num_heads_for_this_ngram):\n",
    "                mod = int(head_vocab_sizes[j])\n",
    "                head_hash = mix % mod\n",
    "                all_hashes.append(head_hash.astype(np.int64, copy=False))\n",
    "        \n",
    "        return np.stack(all_hashes, axis=2)\n",
    "\n",
    "    def hash(self, input_ids):\n",
    "        # 先压缩（原词表映射到新词表）\n",
    "        input_ids = self.compressed_tokenizer(input_ids)\n",
    "        # 存储所有层的hash值\n",
    "        hash_ids_for_all_layers = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            hash_ids_for_all_layers[layer_id] = self._get_ngram_hashes(input_ids, layer_id=layer_id)\n",
    "        return hash_ids_for_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c3743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [[758339, 758341, 758357, 758363, 758383, 758393, 758411, 758431],\n",
       "  [758441, 758449, 758453, 758491, 758501, 758503, 758519, 758521]],\n",
       " 15: [[758551, 758561, 758573, 758579, 758599, 758617, 758629, 758633],\n",
       "  [758671, 758687, 758699, 758707, 758711, 758713, 758729, 758731]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_mapping = NgramHashMapping(\n",
    "            engram_vocab_size=engram_cfg.engram_vocab_size,\n",
    "            max_ngram_size = engram_cfg.max_ngram_size,\n",
    "            n_embed_per_ngram = engram_cfg.n_embed_per_ngram,\n",
    "            n_head_per_ngram = engram_cfg.n_head_per_ngram,\n",
    "            layer_ids = engram_cfg.layer_ids,\n",
    "            tokenizer_name_or_path=engram_cfg.tokenizer_name_or_path,\n",
    "            pad_id = engram_cfg.pad_id,\n",
    "            seed = engram_cfg.seed)\n",
    "\n",
    "hash_mapping.vocab_size_across_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a15c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash_input_ids {1: array([[[755599,  31554, 689651, 733106, 369655, 302899, 739642, 655782,\n",
      "         494078, 275297, 230074, 511023, 747045, 712517,  62185, 692641],\n",
      "        [400072, 438427, 147140, 394998, 365056, 320358, 730452, 183936,\n",
      "          85194,  24976,  14144, 344290,  93743, 735087, 660096, 189452],\n",
      "        [598914, 732143, 362602, 197311, 690520, 603491, 305037,   3385,\n",
      "         116786, 360635, 553188,  65963, 432618, 350759, 107148, 317562],\n",
      "        [141012,  29587, 270597, 121304,  21465, 199878, 750317, 584124,\n",
      "          75068, 554830, 386574, 575367, 654789, 542405, 227831, 640827],\n",
      "        [219573, 634069, 308857, 286706, 118050, 151360, 645674, 143477,\n",
      "         548738, 752325, 183427,  42745, 149888, 707116, 366234, 102811],\n",
      "        [648509,  56564, 191684, 199396, 674053, 294194,  87774, 393810,\n",
      "         428917, 574008,  63545, 181506,   3075,  16603, 410713, 116337],\n",
      "        [155668, 690406,  79215,  10545, 706724, 479947, 197641, 398283,\n",
      "         512050, 476935, 571244, 152594, 676341, 343573, 622083, 645143]]]), 15: array([[[450341, 278297, 101291, 397214, 200168, 636707, 691775, 179274,\n",
      "         728693, 416439, 212766, 633424, 251974, 102979, 671268,  13924],\n",
      "        [216036, 655413, 685333, 482767, 387953, 234772, 161867, 457256,\n",
      "         738864, 706286, 403351, 718572, 727753, 410703,  17616,  46654],\n",
      "        [723008, 460093,  73117, 608211, 472510, 222272, 196473, 673202,\n",
      "         472067, 238494, 461812, 419628, 644404,  59504, 646421, 429469],\n",
      "        [677788, 714899, 673795, 162407, 557410, 598905, 244450, 507453,\n",
      "         711950, 541491, 614609, 161308, 694168, 486558, 344999, 327339],\n",
      "        [311642, 498963, 596348, 275696,  13295, 674575, 137083, 722440,\n",
      "         691980, 162122,  56827, 341564, 741970, 153104,  23535,  10858],\n",
      "        [236341, 650325, 105860, 275919, 439554, 341738, 729287, 612568,\n",
      "         289816, 394288, 440798, 671700, 239810, 456012, 415908, 189460],\n",
      "        [290304,  12960, 527558, 124441,  10220, 600228,  52395, 179931,\n",
      "          28362, 555413,   5535, 325461, 539352, 280381,  46975, 626785]]])}\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([[101, 2000, 2022, 1037, 2204, 2154, 102]])\n",
    "hash_input_ids = hash_mapping.hash(input_ids)\n",
    "print('hash_input_ids', hash_input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2712825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadEmbedding(nn.Module):\n",
    "    def __init__(self, list_of_N: List[int], D: int):\n",
    "        super().__init__()\n",
    "        # list_of_N为每个头的词表列表\n",
    "        self.num_heads = len(list_of_N)\n",
    "        self.embedding_dim = D\n",
    "        \n",
    "        # ngram在每个头内的索引是[0,vocal_size]之间,\n",
    "        # 由于要把所有头的词表放在同一个大Embedding里，所以需要对每个头的索引进行偏移\n",
    "        offsets = [0]\n",
    "        for n in list_of_N[:-1]:\n",
    "            offsets.append(offsets[-1] + n)\n",
    "        \n",
    "        self.register_buffer(\"offsets\", torch.tensor(offsets, dtype=torch.long))\n",
    "        print('offsets:', offsets)\n",
    "        # 所有头的词表大小总和总词表大小\n",
    "        total_N = sum(list_of_N)\n",
    "        print('总词表大小:', total_N)\n",
    "        self.embedding = nn.Embedding(num_embeddings=total_N, embedding_dim=D)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids + self.offsets\n",
    "        output = self.embedding(shifted_input_ids)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f3aca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_of_N: [758339, 758341, 758357, 758363, 758383, 758393, 758411, 758431, 758441, 758449, 758453, 758491, 758501, 758503, 758519, 758521]\n"
     ]
    }
   ],
   "source": [
    "list_of_N = [x for y in hash_mapping.vocab_size_across_layers[1] for x in y]\n",
    "print('list_of_N:', list_of_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56f574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsets: [0, 758339, 1516680, 2275037, 3033400, 3791783, 4550176, 5308587, 6067018, 6825459, 7583908, 8342361, 9100852, 9859353, 10617856, 11376375]\n",
      "总词表大小: 12134896\n",
      "multi_head_embedding: torch.Size([1, 7, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "multi_head_embedding = MultiHeadEmbedding(\n",
    "        list_of_N = [x for y in hash_mapping.vocab_size_across_layers[1] for x in y],\n",
    "        D = engram_cfg.n_embed_per_ngram // engram_cfg.n_head_per_ngram,\n",
    "    )\n",
    "print('multi_head_embedding:', multi_head_embedding(torch.from_numpy(hash_input_ids[1])).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "924a8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engram(nn.Module):\n",
    "    def __init__(self, layer_id):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.hash_mapping = NgramHashMapping(\n",
    "            engram_vocab_size=engram_cfg.engram_vocab_size,\n",
    "            max_ngram_size = engram_cfg.max_ngram_size,\n",
    "            n_embed_per_ngram = engram_cfg.n_embed_per_ngram,\n",
    "            n_head_per_ngram = engram_cfg.n_head_per_ngram,\n",
    "            layer_ids = engram_cfg.layer_ids,\n",
    "            tokenizer_name_or_path=engram_cfg.tokenizer_name_or_path,\n",
    "            pad_id = engram_cfg.pad_id,\n",
    "            seed = engram_cfg.seed,\n",
    "        )\n",
    "        # 每个层对应一个multi_head_embedding\n",
    "        self.multi_head_embedding = MultiHeadEmbedding(\n",
    "            list_of_N = [x for y in self.hash_mapping.vocab_size_across_layers[self.layer_id] for x in y],\n",
    "            D = engram_cfg.n_embed_per_ngram // engram_cfg.n_head_per_ngram,\n",
    "        )\n",
    "        self.short_conv = ShortConv(\n",
    "            hidden_size = backbone_config.hidden_size,\n",
    "            kernel_size = engram_cfg.kernel_size,\n",
    "            dilation    = engram_cfg.max_ngram_size,\n",
    "            hc_mult     = backbone_config.hc_mult,\n",
    "        )\n",
    "        # ngram的数量（2-gram、3-gram等）乘以每个ngram的embedding维度（2*n_embed_per_ngram，n_embed_per_ngram=n_head_per_ngram*D）\n",
    "        engram_hidden_size = (engram_cfg.max_ngram_size-1) * engram_cfg.n_embed_per_ngram\n",
    "        print('engram_hidden_size:', engram_hidden_size)\n",
    "        \n",
    "        # V的投影矩阵\n",
    "        self.value_proj = nn.Linear(engram_hidden_size,backbone_config.hidden_size)\n",
    "        # K的投影矩阵\n",
    "        self.key_projs = nn.ModuleList(\n",
    "            [nn.Linear(engram_hidden_size,backbone_config.hidden_size) for _ in range(backbone_config.hc_mult)]\n",
    "        )\n",
    "        self.norm1 = nn.ModuleList([nn.RMSNorm(backbone_config.hidden_size) for _ in range(backbone_config.hc_mult)])\n",
    "        self.norm2 = nn.ModuleList([nn.RMSNorm(backbone_config.hidden_size) for _ in range(backbone_config.hc_mult)])\n",
    "    \n",
    "    def forward(self,hidden_states,input_ids):\n",
    "        \"\"\"\n",
    "        hidden_states: [B, L, HC_MULT, D]\n",
    "        input_ids: [B, L]\n",
    "        \"\"\"\n",
    "        print('input_ids shape:', input_ids.shape)\n",
    "        # 原始input_ids经过压缩，和多头hash，每个原始token得到多个hash值（该token的2-gram、3gram在不同头对应的索引）\n",
    "        hash_input_ids = torch.from_numpy(self.hash_mapping.hash(input_ids)[self.layer_id])\n",
    "        print('hash_input_ids shape:', hash_input_ids.shape)\n",
    "        # 根据得到的hash索引查找对应索引的embedding\n",
    "        embeddings = self.multi_head_embedding(hash_input_ids).flatten(start_dim=-2)\n",
    "        print('embeddings shape:', embeddings.shape)\n",
    "        gates = []\n",
    "        for hc_idx in range(backbone_config.hc_mult):\n",
    "            # embeddings经过K的投影矩阵得到K\n",
    "            key = self.key_projs[hc_idx](embeddings)\n",
    "            normed_key = self.norm1[hc_idx](key)\n",
    "            query = hidden_states[:,:,hc_idx,:]\n",
    "            # Q由隐藏层得到（包含上下文信息）\n",
    "            normed_query = self.norm2[hc_idx](query)\n",
    "            gate = (normed_key * normed_query).sum(dim=-1) / math.sqrt(backbone_config.hidden_size)\n",
    "            gate = gate.abs().clamp_min(1e-6).sqrt() * gate.sign()\n",
    "            gate = gate.sigmoid().unsqueeze(-1)\n",
    "            print('gate shape:', gate.shape)\n",
    "            gates.append(gate)\n",
    "         \n",
    "        gates = torch.stack(gates,dim=2)\n",
    "        print('gates shape:', gates.shape)\n",
    "        # embeddings经过V的投影矩阵得到V\n",
    "        value = gates * self.value_proj(embeddings).unsqueeze(2)\n",
    "        output = value + self.short_conv(value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ca4f8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsets: [0, 758339, 1516680, 2275037, 3033400, 3791783, 4550176, 5308587, 6067018, 6825459, 7583908, 8342361, 9100852, 9859353, 10617856, 11376375]\n",
      "总词表大小: 12134896\n",
      "engram_hidden_size: 1024\n",
      "input_ids shape: torch.Size([1, 6])\n",
      "hash_input_ids shape: torch.Size([1, 6, 16])\n",
      "embeddings shape: torch.Size([1, 6, 1024])\n",
      "gate shape: torch.Size([1, 6, 1])\n",
      "gate shape: torch.Size([1, 6, 1])\n",
      "gate shape: torch.Size([1, 6, 1])\n",
      "gate shape: torch.Size([1, 6, 1])\n",
      "gates shape: torch.Size([1, 6, 4, 1])\n",
      "torch.Size([1, 6, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "engram = Engram(layer_id=1)\n",
    "hidden_states = torch.randn(1, 6, 4, 1024)\n",
    "input_ids = torch.randint(0, 10000, (1, 6))\n",
    "output = engram(hidden_states, input_ids)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5567ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,layer_id):\n",
    "        super().__init__()\n",
    "        self.attn = lambda x:x\n",
    "        self.moe  = lambda x:x\n",
    "        self.engram = None\n",
    "        if layer_id in engram_cfg.layer_ids:\n",
    "            self.engram = Engram(layer_id=layer_id)\n",
    "    \n",
    "    def forward(self,input_ids,hidden_states):\n",
    "        if self.engram is not None:\n",
    "            hidden_states = self.engram(hidden_states=hidden_states,input_ids=input_ids) + hidden_states\n",
    "        hidden_states = self.attn(hidden_states) + hidden_states\n",
    "        hidden_states = self.moe(hidden_states) + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e36e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offsets: [0, 758339, 1516680, 2275037, 3033400, 3791783, 4550176, 5308587, 6067018, 6825459, 7583908, 8342361, 9100852, 9859353, 10617856, 11376375]\n",
      "总词表大小: 12134896\n",
      "engram_hidden_size: 1024\n",
      "offsets: [0, 758551, 1517112, 2275685, 3034264, 3792863, 4551480, 5310109, 6068742, 6827413, 7586100, 8344799, 9103506, 9862217, 10620930, 11379659]\n",
      "总词表大小: 12138390\n",
      "engram_hidden_size: 1024\n",
      "input_ids shape: torch.Size([1, 13])\n",
      "hash_input_ids shape: torch.Size([1, 13, 16])\n",
      "embeddings shape: torch.Size([1, 13, 1024])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gates shape: torch.Size([1, 13, 4, 1])\n",
      "input_ids shape: torch.Size([1, 13])\n",
      "hash_input_ids shape: torch.Size([1, 13, 16])\n",
      "embeddings shape: torch.Size([1, 13, 1024])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gate shape: torch.Size([1, 13, 1])\n",
      "gates shape: torch.Size([1, 13, 4, 1])\n",
      "✅ Forward Complete!\n",
      "input_ids.shape=torch.Size([1, 13])\n",
      "output.shape=torch.Size([1, 13, 151665])\n"
     ]
    }
   ],
   "source": [
    "LLM = [\n",
    "        nn.Embedding(backbone_config.vocab_size,backbone_config.hidden_size),\n",
    "        *[TransformerBlock(layer_id=layer_id) for layer_id in range(backbone_config.num_layers)],\n",
    "        nn.Linear(backbone_config.hidden_size, backbone_config.vocab_size)\n",
    "    ]\n",
    "\n",
    "text = \"Only Alexander the Great could tame the horse Bucephalus.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(engram_cfg.tokenizer_name_or_path,trust_remote_code=True)\n",
    "input_ids = tokenizer(text,return_tensors='pt').input_ids\n",
    "\n",
    "B,L = input_ids.shape\n",
    "\n",
    "for idx, layer in enumerate(LLM):\n",
    "    if idx == 0:\n",
    "        hidden_states = LLM[0](input_ids)\n",
    "        ## mock hyper-connection\n",
    "        hidden_states = hidden_states.unsqueeze(2).expand(-1, -1, backbone_config.hc_mult, -1)      \n",
    "    elif idx == len(LLM)-1:\n",
    "        ## mock hyper-connection\n",
    "        hidden_states = hidden_states[:,:,0,:] \n",
    "        output = layer(hidden_states)\n",
    "    else:\n",
    "        hidden_states = layer(input_ids=input_ids,hidden_states=hidden_states)\n",
    "\n",
    "print(\"✅ Forward Complete!\")\n",
    "print(f\"{input_ids.shape=}\\n{output.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
