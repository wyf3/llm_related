{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d76e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn import MultiheadAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3823311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix: torch.Tensor, num_iter: int = 20, epsilon: float = 1e-20) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    所有行和为1，所有列和为1，且元素非负。\n",
    "    \"\"\"\n",
    "    # 确保元素非负\n",
    "    K = torch.exp(matrix)\n",
    "    for _ in range(num_iter):\n",
    "        # 行归一化，使每行和为1\n",
    "        K = K / (K.sum(dim=-1, keepdim=True) + epsilon)\n",
    "        # 列归一化，使每列和为1\n",
    "        K = K / (K.sum(dim=-2, keepdim=True) + epsilon)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbe40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mHC(nn.Module):\n",
    "    def __init__(self, dim, n, layer_id):\n",
    "        super(mHC, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "        self.nc = n * dim\n",
    "        self.n2 = n * n\n",
    "        \n",
    "        self.phi = nn.Linear(self.nc, self.n2 + 2 * self.n, bias=False)\n",
    "        # 也可拆分成3个矩阵\n",
    "        # self.phi_pre = nn.Linear(self.nc, self.n, bias=False)\n",
    "        # self.phi_post = nn.Linear(self.nc, self.n, bias=False)\n",
    "        # self.phi_res = nn.Linear(self.nc, self.n2, bias=False)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.ones(3) * 0.01)\n",
    "        # 也可拆分成3个\n",
    "        # self.a_pre = nn.Parameter(torch.ones(1) * 0.01)\n",
    "        # self.a_post = nn.Parameter(torch.ones(1) * 0.01)\n",
    "        # self.a_res = nn.Parameter(torch.ones(1) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n2 + 2 * self.n))\n",
    "        # 也可拆分成3个矩阵\n",
    "        # self.b_pre = nn.Parameter(torch.zeros(self.n))\n",
    "        # self.b_post = nn.Parameter(torch.zeros(self.n))\n",
    "        # self.b_res = nn.Parameter(torch.zeros(self.n2))\n",
    "    \n",
    "    # 不同分支之间信息交互\n",
    "    def width_connection(self, hidden_states):\n",
    "        # 前一层的输出：hidden_states\n",
    "        B, L, N, D = hidden_states.shape  # [B, L, n, dim]\n",
    "        hidden_states_flatten = hidden_states.flatten(2)  # [B, L, n*dim]\n",
    "        r = hidden_states_flatten.norm(dim=-1, keepdim=True) / math.sqrt(self.nc) # [B, L, 1]\n",
    "        \n",
    "        H = self.phi(hidden_states_flatten)  # [B, L, n*n + 2*n]\n",
    "        H_pre = (1/r) * H[:, :, :self.n] * self.a[0] + self.b[0:self.n]  # [B, L, 1] * [B, L, n] + [n, ]\n",
    "        H_post = (1/r) * H[:, :, self.n:self.n*2] * self.a[1] + self.b[self.n:self.n*2]  # [B, L, 1] * [B, L, n] + [n, ]\n",
    "        H_res = (1/r) * H[:, :, self.n*2:] * self.a[2] + self.b[self.n*2:]  # [B, L, 1] * [B, L, n*n] + [n*n, ]\n",
    "        \n",
    "        H_pre = F.sigmoid(H_pre)\n",
    "        H_post = 2 *F.sigmoid(H_post)\n",
    "        \n",
    "        H_res = H_res.reshape(B, L, self.n, self.n) # [B, L, n, n]\n",
    "        H_res = sinkhorn_knopp(H_res)\n",
    "        \n",
    "        H_pre = H_pre.unsqueeze(dim=2) # [B, L, 1, n]\n",
    "        h_pre = torch.matmul(H_pre, hidden_states)  # [B, L, 1, n] @ [B, L, n, dim] = [B, L, 1, dim]\n",
    "        h_res = torch.matmul(H_res, hidden_states)  # [B, L, n, n] @ [B, L, n, dim] = [B, L, n, dim]\n",
    "        \n",
    "        return h_pre, h_res, H_post\n",
    "    \n",
    "    # 不同层之间信息传递，残差连接\n",
    "    def depth_connection(self, h_res, hidden_states, H_post):\n",
    "        # H_post: [B, L, n]\n",
    "        # hidden_states: [B, L, dim]，经过attention或者ffn后的输出\n",
    "        h_post = torch.matmul(H_post.unsqueeze(-1), hidden_states.unsqueeze(-2))# [B, L, n, 1] * [B, L, 1, dim] = [B, L, n, dim]\n",
    "        output = h_post + h_res\n",
    "                    \n",
    "        return output # [B, L, n, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b05cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = torch.randn(1, 6, 4, 64) # [B, L, n, dim]\n",
    "mhc = mHC(dim=64, n=4, layer_id=0)\n",
    "h_pre, h_res, H_post = mhc.width_connection(hidden_states)\n",
    "# h_pre: [B, L, 1, dim]\n",
    "# h_res: [B, L, n, dim]\n",
    "# H_post: [B, L, n]\n",
    "output = mhc.depth_connection(h_res, h_pre.squeeze(-2), H_post)\n",
    "print(output.shape)  # [B, L, n, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f93184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.proj_up = nn.Linear(dim, hidden_dim)\n",
    "        self.proj_down = nn.Linear(hidden_dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.proj_up(x))\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa42e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, n_heads, layer_id, n=4):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attn_mhc = mHC(dim=dim, n=n, layer_id=layer_id)\n",
    "        self.ffn_mhc = mHC(dim=dim, n=n, layer_id=layer_id)\n",
    "        self.attention = MultiheadAttention(embed_dim=dim, num_heads=n_heads, bias=False, batch_first=True)\n",
    "        self.ffn = FFN(dim=dim, hidden_dim=4*dim)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        h_pre, h_res, H_post = self.attn_mhc.width_connection(hidden_states)\n",
    "        # h_pre: [B, L, 1, dim]\n",
    "        attn_output, _ = self.attention(h_pre.squeeze(-2), h_pre.squeeze(-2), h_pre.squeeze(-2))\n",
    "        # attn_output: [B, L, dim]\n",
    "        hidden_states = self.attn_mhc.depth_connection(h_res, attn_output, H_post)  # [B, L, n, dim]\n",
    "        \n",
    "        h_pre, h_res, H_post = self.ffn_mhc.width_connection(hidden_states)\n",
    "        ffn_output = self.ffn(h_pre.squeeze(-2))  # [B, L, dim]\n",
    "        hidden_states = self.ffn_mhc.depth_connection(h_res, ffn_output, H_post)  # [B, L, n, dim]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d21612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderLayer(dim=64, n_heads=4, layer_id=0)\n",
    "hidden_states = torch.randn(1, 6, 4, 64) # [B, L, n, dim]\n",
    "output = decoder(hidden_states)\n",
    "print(output.shape)  # [B, L, n, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ea8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, n_heads, num_layers, n = 4):\n",
    "        super(LLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(dim=dim, n_heads=n_heads, layer_id=i, n=n) for i in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        hidden_states = self.embedding(input_ids)  # (B, L, dim)\n",
    "        hidden_states = hidden_states.unsqueeze(2).expand(-1, -1, 4, -1)  # (B, L, n, dim)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)  # (B, L, n, dim)\n",
    "        output = self.output_layer(hidden_states.mean(dim=2))  # (B, L, vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a39750bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5000])\n"
     ]
    }
   ],
   "source": [
    "model = LLM(vocab_size=5000, dim=64, n_heads=4, num_layers=2, n=4)\n",
    "input_ids = torch.randint(0, 5000, (1, 10))  # (B, L)\n",
    "output = model(input_ids)\n",
    "print(output.shape)  # (B, L, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
