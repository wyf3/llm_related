import gradio as gr
from mcp.client.sse import sse_client
from mcp import ClientSession
from openai import AsyncOpenAI
import json


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚
ä½ å¯ä»¥ä½¿ç”¨ MCP æœåŠ¡å™¨æä¾›çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚
MCP æœåŠ¡å™¨ä¼šåŠ¨æ€æä¾›å·¥å…·ï¼Œä½ éœ€è¦å…ˆæ£€æŸ¥å½“å‰å¯ç”¨çš„å·¥å…·ã€‚

åœ¨ä½¿ç”¨ MCP å·¥å…·æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
1ã€æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·
2ã€æŒ‰ç…§å·¥å…·çš„å‚æ•°è¦æ±‚æä¾›æ­£ç¡®çš„å‚æ•°
3ã€è§‚å¯Ÿå·¥å…·çš„è¿”å›ç»“æœï¼Œå¹¶æ ¹æ®ç»“æœå†³å®šä¸‹ä¸€æ­¥æ“ä½œ
4ã€å·¥å…·å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œæ¯”å¦‚æ–°å¢å·¥å…·æˆ–ç°æœ‰å·¥å…·æ¶ˆå¤±

è¯·éµå¾ªä»¥ä¸‹æŒ‡å—ï¼š
- ä½¿ç”¨å·¥å…·æ—¶ï¼Œç¡®ä¿å‚æ•°ç¬¦åˆå·¥å…·çš„æ–‡æ¡£è¦æ±‚
- å¦‚æœå‡ºç°é”™è¯¯ï¼Œè¯·ç†è§£é”™è¯¯åŸå› å¹¶å°è¯•ç”¨ä¿®æ­£åçš„å‚æ•°é‡æ–°è°ƒç”¨
- æŒ‰ç…§ä»»åŠ¡éœ€æ±‚é€æ­¥å®Œæˆï¼Œä¼˜å…ˆé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
- å¦‚æœéœ€è¦è¿ç»­è°ƒç”¨å¤šä¸ªå·¥å…·ï¼Œè¯·ä¸€æ¬¡åªè°ƒç”¨ä¸€ä¸ªå·¥å…·å¹¶ç­‰å¾…ç»“æœ

è¯·æ¸…æ¥šåœ°å‘ç”¨æˆ·è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹å’Œæ“ä½œæ­¥éª¤ã€‚
"""
     
async def query(query: str, mcp_server_url, model_name, base_url, api_key, temperature):
    
    client = AsyncOpenAI(
            base_url=base_url, api_key=api_key
        )

    async with sse_client(mcp_server_url) as streams:
    
        async with ClientSession(*streams) as session:

            await session.initialize()
            
            response = await session.list_tools()
            messages = [
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            available_tools = [{
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            } for tool in response.tools]

            
            # åˆå§‹åŒ– LLM API è°ƒç”¨
            response = await client.chat.completions.create(
                model=model_name,
                temperature=temperature,
                messages=messages,
                tools=available_tools,
                stream=True
            )
            # message = response.choices[0].message
            full_response = ""
            tool_call_text = ""
          
            while True:
                func_call_list = []
                async for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield tool_call_text, full_response.replace('<think>', "").replace('</think>', "")  # æµå¼è¿”å›å½“å‰ç´¯ç§¯å†…å®¹
                    elif chunk.choices[0].delta.tool_calls:
                        
                        for tcchunk in chunk.choices[0].delta.tool_calls:
                            if len(func_call_list) <= tcchunk.index:
                                func_call_list.append({
                                    "id": "",
                                    "name": "",
                                    "type": "function", 
                                    "function": { "name": "", "arguments": "" } 
                                })
                            tc = func_call_list[tcchunk.index]
                            if tcchunk.id:
                                tc["id"] += tcchunk.id
                            if tcchunk.function.name:
                                tc["function"]["name"] += tcchunk.function.name
                            if tcchunk.function.arguments:
                                tc["function"]["arguments"] += tcchunk.function.arguments
                
                        
                if not func_call_list:
                    break
                
                full_response += 'ğŸ› ï¸ è°ƒç”¨å·¥å…·...\n'
                yield tool_call_text, full_response.replace('<think>', "").replace('</think>', "")
                
                for tool_call in func_call_list:
                    print(tool_call)
                    tool_name = tool_call['function']['name']
                    if tool_call['function']['arguments']:
                        tool_args = json.loads(tool_call['function']['arguments'])
                    else:
                        tool_args = {}

                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    result = await session.call_tool(tool_name, tool_args)
                    # è®°å½•è°ƒç”¨è¯¦æƒ…åˆ°çŠ¶æ€æ 
                    tool_call_text += f"âœ… å·¥å…·è¿”å›: {tool_name}\nå‚æ•°: {tool_args}\nç»“æœ: {str(result.content)}\n---\n"
                    yield tool_call_text, full_response.replace('<think>', "").replace('</think>', "")  # å…ˆæ›´æ–°çŠ¶æ€æ 
                    
                    # å°†å·¥å…·è°ƒç”¨å’Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                    messages.append({
                        "role": "assistant",
                        "tool_calls": [
                            {
                                "id": tool_call['id'],
                                "type": "function",
                                "function": {
                                    "name": tool_name,
                                    "arguments": json.dumps(tool_args)
                                }
                            }
                        ]
                    })
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call['id'],
                        "content": str(result.content)
                    })

                # å°†å·¥å…·è°ƒç”¨çš„ç»“æœäº¤ç»™ LLM
                response = await client.chat.completions.create(
                    model=model_name,
                    temperature=temperature,
                    messages=messages,
                    tools=available_tools,
                    stream=True)
            
                

with gr.Blocks() as demo:
    gr.Markdown("## MCP å®¢æˆ·ç«¯")
    
    # å·¦å³åˆ†æ å¸ƒå±€
    with gr.Row():
        # å·¦ä¾§å‚æ•°è¾“å…¥æ 
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ§  å¤§æ¨¡å‹é…ç½®")
            model_name = gr.Textbox(
                label="æ¨¡å‹åç§°"
            )
            base_url = gr.Textbox(
                label="API åœ°å€"
            )
            api_key = gr.Textbox(
                label="API Key",
                type="password"
            )
            temperature = gr.Number(
                label="æ¸©åº¦",
                value=0.0,
            )
            
            gr.Markdown("### ğŸŒ MCP æœåŠ¡é…ç½®")
            mcp_server_url = gr.Textbox(
                label="MCP æœåŠ¡åœ°å€"
            )
            
            # å·¥å…·è°ƒç”¨çŠ¶æ€é¢æ¿
            tool_status = gr.Textbox(
                label="ğŸ› ï¸ å·¥å…·è°ƒç”¨è®°å½•",
                lines=10,
                interactive=False,
                autoscroll=True,
            )

        # å³ä¾§è¾“å‡ºåŒºåŸŸ
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’¬ äº¤äº’çª—å£")
            result_display = gr.Textbox(
                label="ğŸ§  æ¨¡å‹è¾“å‡º",
                lines=35,
                show_copy_button=True,
            )
    
    # æœ€åº•éƒ¨é—®é¢˜è¾“å…¥åŒº
    with gr.Row():
        query_input = gr.Textbox(
            label="â“ è¾“å…¥ä½ çš„é—®é¢˜",
            placeholder="è¾“å…¥é—®é¢˜åç‚¹å‡»ç”ŸæˆæŒ‰é’®...",
            scale=4
        )
        generate = gr.Button(
            "ğŸš€ å¼€å§‹ç”Ÿæˆ",
            scale=1,
            variant="primary"
        )
    
    generate.click(fn=query, inputs=[query_input, mcp_server_url, model_name, base_url, api_key, temperature], outputs=[tool_status, result_display])
    

    
    
    
if __name__ == "__main__":
    demo.queue().launch(server_name='0.0.0.0', allowed_paths=['./'])

